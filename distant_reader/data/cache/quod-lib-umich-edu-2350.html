<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01+RDFa 1.1//EN" "http://www.w3.org/MarkUp/DTD/html401-rdfa11-1.dtd">
<html xmlns:ns_1="xmlns" ns_1:dc="http://purl.org/dc/elements/1.1/" class="" lang="en">
<head profile="http://purl.org/NET/erdf/profile">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="canonical" href="http://dx.doi.org/10.3998/weave.12535642.0001.101">
<link rel="schema.dc" href="http://purl.org/dc/elements/1.1/">
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://quod.lib.umich.edu/w/weave/longfeed.xml">
<title>Improving Library User Experience with A/B Testing: Principles and Process</title>

          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/jquery-ui.css">
          <!--<![endif]-->
        
          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/mpubs.css">
          <!--<![endif]-->
        
          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubserials/styles/mpubs.css">
          <!--<![endif]-->
        
          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/static.css">
          <!--<![endif]-->
        
          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/footer.css">
          <!--<![endif]-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/print.css" media="print">

          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/m/mpubs/styles/custom.css">
          <!--<![endif]-->
        
          <!--[if ! lte IE 6]><!-->
        <link rel="stylesheet" type="text/css" href="https://quod.lib.umich.edu/w/weave/styles/custom.css">
          <!--<![endif]-->
        
      <!--[if ! lte IE 6]><!-->
    
      <!--<![endif]-->
    <!--[if lte IE 6]>
      <link rel="stylesheet" href="/m/mpubserials/styles/external/universal-ie6.1.1.css" media="screen, projection">
      <link rel="stylesheet" href="/m/mpubserials/styles/ie6.css" media="screen, projection">
    <![endif]-->
    <script type="text/javascript" src="https://quod.lib.umich.edu/m/mpubs/js/jquery.js"></script>
    <script type="text/javascript" src="https://quod.lib.umich.edu/m/mpubs/js/jquery-ui.js"></script>
    <script type="text/javascript" src="https://quod.lib.umich.edu/m/mpubs/js/library.js"></script>
    <meta name="DC.title" content="Improving Library User Experience with A/B Testing: Principles and Process">
    <meta name="citation_title" content="Improving Library User Experience with A/B Testing: Principles and Process">
    <meta name="DC.creator" content="Young, Scott W. H.">
    <meta name="citation_author" content="Young, Scott W. H.">
    <meta name="DC.date" content="2014">
    <meta name="DC.issued" content="2014">
    <meta name="citation_publication_date" content="2014">
    <meta name="citation_online_date" content="2014">
    <meta name="DC.date" content="2014-08">
    <meta name="DC.relation.ispartof" content="Weave: Journal of Library User Experience">
    <meta name="citation_journal_title" content="Weave: Journal of Library User Experience">
    <meta name="citation_issn" content="2333-3316">
    <meta name="DC.citation.volume" content="1">
    <meta name="citation_volume" content="1">
    <meta name="DC.citation.issue" content="1">
    <meta name="citation_issue" content="1">
    <meta name="DC.identifier" content="10.3998/weave.12535642.0001.101">
    <meta name="citation_doi" content="10.3998/weave.12535642.0001.101">
    <meta name="DC.publisher" content="Michigan Publishing, University of Michigan Library">
    <meta name="citation_publisher" content="Michigan Publishing, University of Michigan Library">
<link href="https://fonts.googleapis.com/css?family=Julius+Sans+One" rel="stylesheet" type="text/css">
    <script type="text/javascript" src="https://quod.lib.umich.edu/w/weave/js/library.js"></script>
</head>
<body class="encodedText">
<div id="skipnav"><ul>
<li><a href="quod-lib-umich-edu-2350.html#articlebody">Skip to main content</a></li>
<li><a href="quod-lib-umich-edu-2350.html#q1">Skip to quick search</a></li>
<li><a href="quod-lib-umich-edu-2350.html#globalnav">Skip to global navigation</a></li>
</ul></div>
<span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fquod.umdl.umich.edu&amp;rft.title=Weave%3A+Journal+of+Library+User+Experience&amp;rft.jtitle=Weave%3A+Journal+of+Library+User+Experience&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Improving+Library+User+Experience+with+A%2FB+Testing%3A+Principles+and+Process&amp;rft.au=Young,+Scott+W.+H.&amp;rft.date=2014&amp;rft_id=http%3A%2F%2Fhdl.handle.net%2F2027%2Fspo.12535642.0001.101&amp;rft_id=info%3Adoi%2Fhttp%3A%2F%2Fdx.doi.org%2F10.3998%2Fweave.12535642.0001.101&amp;rft.volume=1&amp;rft.issue=1&amp;rft.issn=2333-3316"></span><div class="masthead-wrapper">
<div id="masthead">
<a name="top"></a><div id="logo">
<h1><a href="http://weaveux.org" target="_top"><img src="https://quod.lib.umich.edu/w/weave/graphics/logo.png" alt="Weave Journal of Library User Experience" title="Weave" width="200"><span class="journal-subtitle">Journal of Library User Experience</span></a></h1>
<p>Weave is an open-access, peer-reviewed journal for Library User Experience professionals published by Michigan Publishing.</p>
</div>
<div id="quicksearchwrapper"><form method="get" action="https://quod.lib.umich.edu/w/weave" id="simplesearch">
<input type="hidden" name="type" value="simple"><input type="hidden" name="rgn" value="full text"><label for="quicksearch" class="hide">Quick search:</label><input type="text" name="q1" id="q1" title="quick search" placeholder="quick search"><input type="submit" value="Search"><div id="quicksearchtext"></div>
</form></div>
</div>
<div id="globalactions"><div id="main_nav"><ul role="navigation">
<li><a href="http://weaveux.org" target="_top">Current Issue: Vol. 2, Issue 1</a></li>
<li><a href="https://quod.lib.umich.edu/w/weave/12535642.*" target="_top">Archive</a></li>
<li><a href="https://quod.lib.umich.edu/w/weave/download-epub" target="_top">Download EPUBs</a></li>
<li><a href="http://weaveux.org/submit" target="_top">Submit to Weave</a></li>
</ul></div></div>
</div>
<div id="wrapper">
<div class="maincontent"><div id="article"><div id="articlebody" name="articlebody">
<!--false--><div class="content-top">
<div id="header">
<h1 class="title" property="dc:title">Improving Library User Experience with A/B Testing: Principles and Process<br>
</h1>
<div class="authors" property="dc:creator" content="Young, Scott W. H.">
<div class="author">Scott W. H. Young</div>
<div class="author-notes">Montana State University</div>
</div>
<a href="quod-lib-umich-edu-2350.html#end-of-header" class="hide">Skip other details (including permanent urls, DOI, citation information)</a><div class="periodical">
<a href="https://quod.lib.umich.edu/w/weave/12535642.0001?rgn=main;view=fulltext">Volume 1</a>, <a href="https://quod.lib.umich.edu/w/weave/12535642.0001.1*?rgn=main;view=fulltext">Issue 1</a>, <span property="dc:date">2014</span>
</div>
<div id="doi">
<span class="metalabel">DOI</span>: <a href="http://dx.doi.org/10.3998/weave.12535642.0001.101">http://dx.doi.org/10.3998/weave.12535642.0001.101</a>
</div>
<div id="licenseinfo">
<a xmlns:str="http://exslt.org/strings" rel="license" id="licenseicon" target="_blank" href="http://creativecommons.org/licenses/by/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/3.0/80x15.png"></a>  <a id="licenselink"></a><div xmlns:str="http://exslt.org/strings" id="licensehover" class="displayNone"><span>Creative Commons 3.0 Attribution License</span></div>
</div>
<div id="licensetext" title="Permissions">
<p><span id="licensetextlabel"><span class="metalabel">Permissions</span>: </span>This work is licensed under a Creative Commons Attribution 3.0 License. Please contact mpub-help@umich.edu to use this work in a way not covered by the license.</p>
<p>For more information, read Michigan Publishing's <a href="http://wiki.publishing.umich.edu/Access_and_Usage_Policy" target="_blank">access and usage policy</a>.</p>
</div>
</div>
<a name="end-of-header"></a><div id="extras" class="popout">
<ul class="extras">
<li class="print"><a class="btn" title="Print article" href="javascript:window.print()">Print</a></li>
<li class="share">
<span class="btn">Share+</span><ul class="social">
<li><a href="http://twitter.com/intent/tweet?text=Improving%20Library%20User%20Experience%20with%20A/B%20Testing:%20Principles%20and%20Process&amp;url=http://dx.doi.org/10.3998/weave.12535642.0001.101" title="Share on Twitter">Twitter
            	</a></li>
<li><a href="http://www.facebook.com/sharer.php?u=http://dx.doi.org/10.3998/weave.12535642.0001.101&amp;t=Improving%20Library%20User%20Experience%20with%20A/B%20Testing:%20Principles%20and%20Process" title="Share on Facebook">Facebook
            	</a></li>
<li><a href="http://www.reddit.com/submit?url=http://dx.doi.org/10.3998/weave.12535642.0001.101" title="Submit to Reddit">Reddit
            	</a></li>
<li><a href="http://www.mendeley.com/import/?url=http://dx.doi.org/10.3998/weave.12535642.0001.101" title="Save to Mendeley">Mendeley
            	</a></li>
</ul>
</li>
</ul>
<div class="altmetric-embed" data-badge-type="1" data-badge-popover="bottom" data-hide-no-mentions="true" data-doi="10.3998/weave.12535642.0001.101"></div>
</div>
<span id="header-clear"></span>
</div>
<div>
<p class="prelim">This paper was refereed by Weave's peer reviewers.</p>
<a name="2"></a><h2>Abstract</h2>
<p class="prelim">
This paper demonstrates how user interactions can be measured and evaluated with A/B testing, a user experience research methodology. A/B testing entails a process of controlled experimentation whereby different variations of a product or service are served randomly to users in order to determine the highest performing variation. This paper describes the principles of A/B testing and details a practical web-based application in an academic library. Data collected and analyzed through this A/B testing process allowed the library to initiate user-centered website changes that resulted in increased website engagement and improved user experience. A/B testing is presented as an integral component of a library user experience research program for its ability to provide quantitative user insights into known UX problems.
</p>
<a name="3"></a><h2>Introduction</h2>
<div class="q1-epig">
<div class="line"> “Users must be treated as co-developers.” – Tim O’Reilly, 2009</div>
</div>
<p>Through the process of A/B testing, user experience (UX) researchers can make iterative user-centered, data-driven decisions for library products and services. The “A/B test” and its extension “A/B/n test” represent shorthand notation for a simple controlled experiment, in which users are randomly served one of two or more variations of a product or service: control (A), variation (B), and any number of additional variations (n), where the variations feature a single isolated design variable. With proper experiment design, the highest performing variation can be identified according to predefined metrics (Kohavi, 2007). Within a diverse framework of UX research methodologies, A/B testing offers a productive technique for answering UX questions.</p>
<p>A/B testing has been a tool of user interface designers and user experience researchers since at least 1960, when engineers at Bell Systems experimented with several different versions of the touch-tone phone (Deininger, 1960). The Bell Systems experiment was framed by a user-centered research question: "What are the desirable characteristics of pushbuttons for use in telephone sets?" UX issues such as button size, button arrangement, lettering, button force, and button feedback were tested with Bell Systems employees, who were then asked which of each variation was preferred most and least. In designing this experiment, Deininger notes, “Perhaps the most important factor in the information processing is the individual himself” (p. 1009). Today major companies such as Google, Amazon, Etsy, and Twitter frequently use experiments to make data-driven design decisions.<span class="ptr" id="N1-ptr1"><a href="quod-lib-umich-edu-2350.html#N1">[1]</a></span> In accordance with Google's company philosophy, engineers use experimentation to understand ever-changing user expectations and behavior (Tang, 2010, p. 23).<span class="ptr" id="N2-ptr1"><a href="quod-lib-umich-edu-2350.html#N2">[2]</a></span> For Google, "experimentation is practically a mantra" (Tang, 2010, p.17). </p>
<p>This approach is widely used because the data derived from A/B testing provides direct evidence of user behavior in support of the concept known as “perpetual beta.” In the pursuit of perpetual beta, a product or service remains under regular review, with feedback from users cycling through an iterative design process that continually seeks to improve the UX of that product or service. User-centered research such as A/B testing finds a natural fit within the model of perpetual beta, where a culture of user-driven revision helps create products and services that meet current user expectations.</p>
<p>  In its overall goal of providing insights into user behaviors, A/B testing is similar to other widely-practiced UX research methodologies such as usability testing and heuristics evaluation. The essential value and distinguishing characteristic of A/B testing is its ability to provide quantitative user feedback for known UX problems. Whereas usability testing is designed to reveal previously-unknown UX problems, A/B testing is designed to reveal the optimal solution from among a set of alternative solutions to an already-known UX problem. Since the approach is most suitable for research contexts in which a specific design question can be predetermined, the question-answer structure that defines A/B testing also defines the boundaries of its usefulness. A/B testing is most effective when a clear design inquiry can be formulated in tandem with quantitatively measurable results. In this way, A/B testing can serve as an integral component to a comprehensive UX research program to collect user data and to design for user experiences that meet user expectations. The case study presented in this paper demonstrates the technique as applied to a library web design problem.</p>
<a name="4"></a><h2>Literature Review</h2>
<p>Much of the existing research that explores the statistical structures associated with A/B testing can be found within computer science literature, with specific investigation of sampling techniques, randomization algorithms, assignment methods, and raw data collection (Cochran, 1977; Cox, 2000; Johnson, 2002; Kohavi 2007, 2009; Borodovsky, 2011). The concept of A/B testing has also been present in business marketing literature for several years, with a strong focus on e-commerce goals such increasing click-through rates of ads and conversion rate of customers (Arento, 2010; McGlaughlin, 2006; New Media Age, 2010). A wide-ranging investigation of the UX of commercial websites is also represented in the business literature. This pursuit is couched in the business terms of return-on-investment, customer conversions, and consumer behavior, with a view towards the usability of online commercial transactions (Wang, 2007; Casaló, 2010; Lee S., 2010; Finstad, 2010; Fernandez 2011; Tatari, 2011; Lee Y., 2012; Belanche, 2012). While librarians may not focus so intently on the practice of e-commerce, the substantial user-focused research found in the business literature is instructive for understanding online user behaviors in general.</p>
<p>The library science literature offers no substantial discussion of A/B testing. Commonalities with computer science and business marketing literature can be found in the shared recognition of the value of user feedback and the objective of designing for user experiences that meet user expectations. The library literature offers extensive discussion of UX and user-centered design for a range of library products and services, including websites (Bordac 2008; Kasperek, 2011; Swanson, 2011), library subject guides (Sonsteby, 2013), and search &amp; web-scale discovery (Gross, 2011; Lown, 2013). In 2008 S. Liu (14) offered a straightforward recommendation for library user experience online, "future academic library Web sites should...respond to users’ changing needs." Within libraries, usability studies have been the dominant method for accessing user perspectives and for understanding user behaviors and expectations. Other methods include ethnographic studies (Wu, 2011; Khoo, 2012; Connaway, 2013), eye tracking (Lorigo 2008; Höchstötter, 2009), and data mining and bibliomining (Hajek, 2012). The thrust of these studies converges around an understanding of UX research fundamentally guided by three questions: What do users think they will get? What do users actually get? How do they feel about that? The A/B testing process, when used in concert with these other methodologies, can serve as an integral component of a UX research program by introducing a mechanism for identifying the best answer from among a range of alternative answers to a UX question. </p>
<a name="5"></a><h2>A/B Methodology &amp; Case Study</h2>
<p>The A/B testing process follows an adapted version of the scientific method, in which a hypothesis is made and then tested for validity through the presentation of design variations to the user. The testing process then measures the differences in user reactions to the variations using predefined metrics. For the experiment design to be effective, the central research question must be answerable and the results must be measurable. A successful framework for an A/B test follows these steps: </p>
<ol class="numbered">
<li>Define a research question</li>
<li>Refine the question with user interviews</li>
<li>Formulate a hypothesis, identify appropriate tools, and define test metrics</li>
<li>Set up and run experiment</li>
<li>Collect data and analyze results</li>
<li>Share results and make decision</li>
</ol>
<p>These six steps comprise the A/B testing process and together provide the experiment design with clarity and direction. The case study presented below follows a line of inquiry around a library website and employs web analytics software to build measurable data that answers the initial research question. While the focus of the case study is a website, the essential principles and process of A/B testing may be applied in any context characterized by a known UX problem, a definite design question, and relevant measurable user data.</p>
<div class="textindentlevelx">
<a name="5.1"></a><h3><span class="rend-i">Step 1: Define a research question</span></h3>
<p>This crucial first step sets a course for the A/B test by identifying an general line of inquiry for the experiment. The research question is shaped from a known UX problem, drawing on existing user feedback such as that derived from surveys, interviews, focus groups, information architecture tree testing, paper prototyping, usability testing, or website analytics. In the example of this case study, a research question was developed from user feedback received through website analytics. Following a review of the website analytics for the library’s homepage in the spring of 2013, it was apparent that the <span class="rend-i">Interact</span> category of content on the web page was neglected by users.</p>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000001.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000001.png" alt="Fig. 1: Library Homepage - April 2013" title="Fig. 1: Library Homepage - April 2013" class="thumbfigure"></a><div class="head">Fig. 1: Library Homepage - April 2013</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000002.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000002.png" alt="Fig. 2: Library Homepage Click Data - April 3-April 10, 2013" title="Fig. 2: Library Homepage Click Data - April 3-April 10, 2013" class="thumbfigure"></a><div class="head">Fig. 2: Library Homepage Click Data - April 3-April 10, 2013</div>
</div>
<p>During the sample period from April 3, 2013 - April 10, 2013, which included 10,819 visits to the library homepage<span class="ptr" id="N3-ptr1"><a href="quod-lib-umich-edu-2350.html#N3">[3]</a></span>, there was a large disparity among the three main content categories. The click-through rate<span class="ptr" id="N4-ptr1"><a href="quod-lib-umich-edu-2350.html#N4">[4]</a></span> for <span class="rend-i">Find</span> was 35%, <span class="rend-i">Request</span> was 6%, and <span class="rend-i">Interact</span> was 2%. This observation prompted a question: "Why are <span class="rend-i">Interact</span> clicks so low?" At this time the content beneath <span class="rend-i">Interact</span> included links to Reference Services, Instruction Services, Subject Liaisons, Writing Center, About, Staff Directory, Library FAQ, Give to the Library, and Floor Maps. The library’s web committee surmised that introducing this category with the abstract term "Interact" added difficulty and confusion for users trying to navigate into the library website homepage. Four different category titles were then proposed as variations to be tested: <span class="rend-i">Connect</span>, <span class="rend-i">Learn</span>, <span class="rend-i">Help</span>, and <span class="rend-i">Services</span>. </p>
</div>
<div class="textindentlevelx">
<a name="5.2"></a><h3><span class="rend-i">Step 2: Refine the question with user interviews</span></h3>
<p>Interviews with users regarding the different variations can help refine the A/B test. This qualitative step serves as a small-scale pre-test to confirm that the experiment variations are different enough that feedback through the A/B test itself will lead to meaningful results. Variations that are too similar will be problematic for the experiment design by not allowing for a clear winner to emerge. For the purposes of this case study, ad hoc conversations with 3 undergraduate students were conducted with a guerilla-style approach.<span class="ptr" id="N5-ptr1"><a href="quod-lib-umich-edu-2350.html#N5">[5]</a></span> Questions were designed to provide insight into the user expectations of the library homepage’s category titles and category page. They included:</p>
<ul class="bulleted">
<li>Have you previously clicked on <span class="rend-i">Interact</span>?</li>
<li>What content do you expect to see after you select <span class="rend-i">Interact</span>?</li>
<li>Does <span class="rend-i">Interact</span> accurately describe the content that you find after selecting <span class="rend-i">Interact</span>?</li>
<li>Which word best describes this category? <span class="rend-i">Interact</span>? <span class="rend-i">Connect</span>? <span class="rend-i">Learn</span>? <span class="rend-i">Help</span>? <span class="rend-i">Services</span>?</li>
</ul>
<p>Below are selected excerpts from student responses:</p>
<div class="q1-block">
<p>Sophomore student:</p>
<ul class="bulleted">
<li>"I didn't know that 'About' was under <span class="rend-i">Interact</span>.'"</li>
<li>"<span class="rend-i">Learn</span> doesn't work.”</li>
<li>"<span class="rend-i">Connect</span> is too vague and too close to <span class="rend-i">Interact</span>."</li>
<li> "<span class="rend-i">Services</span> is more accurate. <span class="rend-i">Help</span> is stronger.”</li>
<li>"Floor maps seem odd here."</li>
<li>In order of preferences of the choices, this student responded: <span class="rend-i">Help</span>, <span class="rend-i">Services</span>, <span class="rend-i">Interact</span>, <span class="rend-i">Connect</span>, <span class="rend-i">Learn</span>
</li>
</ul>
<p>Junior student: </p>
<ul class="bulleted"><li>"I am not a native English speaker, so I look for strong words. I look for help, so <span class="rend-i">Help</span> is the best, then <span class="rend-i">Services</span> too."</li></ul>
<p>Senior student: </p>
<ul class="bulleted">
<li>"I've never felt the need to click on <span class="rend-i">Interact</span>. What am I interacting with? I guess the library?"</li>
<li>"I never knew floor maps were there, but I have wondered before where certain rooms were."</li>
<li>"<span class="rend-i">Help</span> makes sense. When I'm in the library, and I think I need help, it would at least get me to click there to find out what sort of help there is."</li>
<li>"<span class="rend-i">Services</span> also works."</li>
<li>"<span class="rend-i">Learn</span> doesn't really work. I just think, What am I learning? I think of reading a book or something."</li>
<li>"<span class="rend-i">Connect</span> is better than <span class="rend-i">Interact</span>, but neither are very good."</li>
<li>In order of preferences of the choices, this student responded: <span class="rend-i">Help</span>, <span class="rend-i">Services</span>, <span class="rend-i">Connect</span>, <span class="rend-i">Interact</span>, <span class="rend-i">Learn</span>
</li>
</ul>
</div>
<p>From these brief interviews, insights about the expectations of users come into focus. These interviews indicated that the experiment variables—<span class="rend-i">Interact</span>, <span class="rend-i">Connect</span>, <span class="rend-i">Learn</span>, <span class="rend-i">Help</span>, <span class="rend-i">Services</span>—were likely to provide meaningful differences during the experimentation period and that either <span class="rend-i">Help</span> or <span class="rend-i">Services</span> was likely be the highest performing variation. While these users indicated that the five different options would be adequately distinct for this test, users may instead indicate that the experiment variations are too alike. Users might also suggest their own unique variations that could be included in the experiment design. In either of these cases, the UX researcher may wish to further refine the variables and repeat this step.</p>
</div>
<div class="textindentlevelx">
<a name="5.3"></a><h3><span class="rend-i">Step 3: Formulate a hypothesis, identify appropriate tools, and define test metrics</span></h3>
<p>As a counterpart to the general UX research question in Step 1, the hypothesis of Step 3 proposes a specific answer that can be measured with metrics relevant to that question. The hypothesis provides a focal point for the research question by precisely expressing the expected user reactions to the experiment variations. With the combined data from web analytics and user interviews for this case study, the library web committee formed the following experiment hypothesis: a homepage with <span class="rend-i">Help</span> or <span class="rend-i">Services</span> will generate increased website engagement compared to <span class="rend-i">Learn</span>, <span class="rend-i">Connect</span>, and <span class="rend-i">Interact</span>. With research question centered around this hypothesis, metrics relevant to website engagement must be identified that will enable the hypothesis to be tested for validity.</p>
<p> As this case study centers on a website-based A/B test, the experiment was conducted using the web analytics software Google Analytics and Crazy Egg.<span class="ptr" id="N6-ptr1"><a href="quod-lib-umich-edu-2350.html#N6">[6]</a></span> The case study objective was to understand which category title communicated its contents most clearly to users and which category title resulted in users following through further into the library website. Three key web-based metrics were consequently used for this experiment: click-through rate for the homepage, drop-off rate for the category pages, and homepage-return rate for the category pages. Click-through rate was selected as a measure of the initial ability of the category title to attract users. Drop-off rate, which is available through the Google Analytics Users Flow and indicates the percentage of users who leave the site from a given page, was selected as a measure of the ability of the category page to meet user expectations. Homepage-return rate is a custom metric formulated for this experiment. This metric, also available through the Google Analytics Users Flow, measures the percentage of users who navigated from the library homepage to the category page, then returned back to the homepage. This sequence of actions provides clues as to whether a user discovered the desired option on the category page; if not, the user would likely then return to the homepage to continue navigation. Homepage-return rate was therefore selected as a measure of the ability of the category page to meet user expectations. The combined feedback from these three metrics provides multiple points of view into user behavior that together will help validate the hypothesis. </p>
</div>
<div class="textindentlevelx">
<a name="5.4"></a><h3><span class="rend-i">Step 4: Set up and run experiment</span></h3>
<p>A/B tests operate most quickly and efficiently when variations reflect a subtle and iterative design progression. Experiments that intend to test stark design differences, on the other hand, may result in a diminished user experience and a less efficient testing process.  Consider for instance an A/B test that seeks to find the optimal layout of a web page. If the experiment were to include significant design changes, then users who encountered different variations might become confused, page navigation would become difficult, and the user experience would likely suffer. For A/B tests that feature such stark differences, the experiment designers may wish to include less than 100% of users so as to reduce the risk of user confusion. With tests that feature more finely-drawn design changes, users are likely to experience only minimal disruption during the course of the experiment. For A/B tests that feature minor differences, the experiment designers may wish to include 100% of users. Tests that reflect iterative design changes and that include all users will lead to faster results, as a greater number of users included in the experiment will yield more significant results in a shorter period of time.</p>
<p>Since the test variables in this case study were largely non-disruptive, 100% of website visitors were included for a duration of three weeks, from May 29, 2013 - June 18, 2013. Based on our typical web traffic, this period of time allowed us to collect enough data for a clear winner to emerge. The experiment was designed to ensure that each user visited a variation with an equal level of randomization and that each variation received an approximately equal number of user visits. Other A/B test designs may employ alternative sampling techniques, and the exact mechanics of an experiment design will vary according to the tools used. For example, Google Analytics offers an alternate experiment design known as multi-armed bandit, in which the randomization distribution is updated as the experiment progresses so that web traffic is increasingly directed towards higher-performing variations. This approach is often favored in e-commerce A/B tests where a profit calculation may determine that low-performing variations only be minimally served to users.</p>
<p>Google Analytics includes a client-side experimentation tool called Experiments, a powerful option that provides the randomization mechanics for website-based A/B testing.<span class="ptr" id="N7-ptr1"><a href="quod-lib-umich-edu-2350.html#N7">[7]</a></span> Experiment settings within Google Analytics control the percentage of traffic to be included in the experiment and the time duration for the experiment.<span class="ptr" id="N8-ptr1"><a href="quod-lib-umich-edu-2350.html#N8">[8]</a></span> As a supplement to Google Analytics, Crazy Egg collects user click data and generates tabular and visual reports of user behavior. While Crazy Egg itself is not required to run A/B tests, it was included in this case study to provide a fuller set of user click data than is available through Google Analytics alone.</p>
<p>With the case study hypothesis formulated, metrics defined, and relevant tools identified, the web committee created variation pages and the experiment was set up using Google Analytics and Crazy Egg. Five different variations of the library homepage ran concurrently on the website, each served randomly and in real-time to website visitors using Google Analytics. The original page and each variation were given sequential names and were located within the same directory. </p>
<table border="1" BORDER="1" class="xmltable">
<tr>
<th class="xmltd">Experimentation Name</th>
<th class="xmltd">Category Title</th>
<th class="xmltd">URL</th>
</tr>
<tr>
<td class="xmltd">Control</td>
<td class="xmltd"><span class="rend-i">Interact</span></td>
<td class="xmltd"><span class="ref"><a href="http://www.lib.montana.edu/index.php" target="_blank">www.lib.montana.edu/index.php</a></span></td>
</tr>
<tr>
<td class="xmltd">Variation 1</td>
<td class="xmltd"><span class="rend-i">Connect</span></td>
<td class="xmltd"><span class="ref"><a href="http://www.lib.montana.edu/index2.php" target="_blank">www.lib.montana.edu/index2.php</a></span></td>
</tr>
<tr>
<td class="xmltd">Variation 2</td>
<td class="xmltd"><span class="rend-i">Learn</span></td>
<td class="xmltd"><span class="ref"><a href="http://www.lib.montana.edu/index3.php" target="_blank">www.lib.montana.edu/index3.php</a></span></td>
</tr>
<tr>
<td class="xmltd">Variation 3</td>
<td class="xmltd"><span class="rend-i">Help</span></td>
<td class="xmltd"><span class="ref"><a href="http://www.lib.montana.edu/index4.php" target="_blank">www.lib.montana.edu/index4.php</a></span></td>
</tr>
<tr>
<td class="xmltd">Variation 4</td>
<td class="xmltd"><span class="rend-i">Services</span></td>
<td class="xmltd"><span class="ref"><a href="http://www.lib.montana.edu/index5.php" target="_blank">www.lib.montana.edu/index5.php</a></span></td>
</tr>
</table>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000003.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000003.png" alt="Fig. 3: Control - Interact" title="Fig. 3: Control - Interact" class="thumbfigure"></a><div class="head">Fig. 3: Control - <span class="rend-i">Interact</span>
</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000004.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000004.png" alt="Fig. 4: Variation 1 - Connect" title="Fig. 4: Variation 1 - Connect" class="thumbfigure"></a><div class="head">Fig. 4: Variation 1 - <span class="rend-i">Connect</span>
</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000005.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000005.png" alt="Fig. 5: Variation 2 - Learn" title="Fig. 5: Variation 2 - Learn" class="thumbfigure"></a><div class="head">Fig. 5: Variation 2 - <span class="rend-i">Learn</span>
</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000006.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000006.png" alt="Fig. 6: Variation 3 - Help" title="Fig. 6: Variation 3 - Help" class="thumbfigure"></a><div class="head">Fig. 6: Variation 3 - <span class="rend-i">Help</span>
</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000007.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000007.png" alt="Fig. 7: Variation 4 - Services" title="Fig. 7: Variation 4 - Services" class="thumbfigure"></a><div class="head">Fig. 7: Variation 4 - <span class="rend-i">Services</span>
</div>
</div>
</div>
<div class="textindentlevelx">
<a name="5.5"></a><h3><span class="rend-i">Step 5: Collect data and analyze results</span></h3>
<p>With the hypothesis defined, metrics identified, and experiment launched, the test itself can run its course for its defined duration. Experiment visits for this case study reflected a typical number of visits for this time period, and at the end of the three-week experiment duration, data for the three key metrics were analyzed. Click-through rate analysis was organized into five categories according to the primary actions of the library homepage: clicks into <span class="rend-i">Search</span>, clicks into <span class="rend-i">Find</span>, clicks into <span class="rend-i">Request</span>, clicks into the variable category title, and clicks into <span class="rend-i">Other</span>, defined as all other entry points on the page.</p>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000008.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000008.png" alt="Fig. 8: Experiment Click-through rates — Control Page, Interact" title="Fig. 8: Experiment Click-through rates — Control Page, Interact" class="thumbfigure"></a><div class="head">Fig. 8: Experiment Click-through rates — Control Page, <span class="rend-i">Interact</span>
</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000009.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000009.png" alt="Fig. 9: Experiment Click-through rates — Variation Pages" title="Fig. 9: Experiment Click-through rates — Variation Pages" class="thumbfigure"></a><div class="head">Fig. 9: Experiment Click-through rates — Variation Pages</div>
</div>
<div class="figure two-col">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000010.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000010.png" alt="Fig. 10: Experiment Click-through Rates" title="Fig. 10: Experiment Click-through Rates" class="thumbfigure"></a><div class="head">Fig. 10: Experiment Click-through Rates</div>
</div>
<div class="figure two-col">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000011.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000011.png" alt="Fig. 11: Experiment Drop-off Rates" title="Fig. 11: Experiment Drop-off Rates" class="thumbfigure"></a><div class="head">Fig. 11: Experiment Drop-off Rates</div>
</div>
<div class="figure">
<a href="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000012.png"><img src="https://quod.lib.umich.edu/w/weave/images/12535642.0001.101-00000012.png" alt="Fig. 12: Experiment Homepage-return Rates" title="Fig. 12: Experiment Homepage-return Rates" class="thumbfigure"></a><div class="head">Fig. 12: Experiment Homepage-return Rates</div>
</div>
<p>“Variation 4 - <span class="rend-i">Services</span>” was the highest performing across Click-through rate, Drop-off rate, and Homepage-return rate.<span class="ptr" id="N9-ptr1"><a href="quod-lib-umich-edu-2350.html#N9">[9]</a></span> This variation performed exceptionally well in Drop-off rate and Homepage-return rate. During the experiment period, of those visitors to this variation who clicked into the category page, 0% returned to the homepage and 0% dropped off the page. These unusually low figures would likely not be replicated over the long-term. When evaluated comparatively, however, “Variation 4 - <span class="rend-i">Services</span>” emerged as the most successful variation according to the experiment metrics. When these results were combined with the user interview responses from Step 2, which indicated that “Variation 4 - <span class="rend-i">Services</span>” or “Variation 3 - <span class="rend-i">Help</span>” would be the most successful variation, “Variation 4 - <span class="rend-i">Services</span>” was identified as the overall highest-performing variation. </p>
<p>While the experiment described in this case study produced a successful test, not every test will produce a clear winner. Ambiguous outcomes may result from poor experiment design or a misconfigured test set up. In cases where no apparent winner emerges, the UX researcher can follow the A/B testing steps to refocus the research question, rework the design variables, or retest the variations with an altered experiment design. If a process of refinement continues to produce no meaningful results, the central research question may be more constructively approached from an alternative UX research perspective. </p>
</div>
<div class="textindentlevelx">
<a name="5.6"></a><h3><span class="rend-i">Step 6: Share results and make decision</span></h3>
<p>An essential final step in the A/B testing process is to convert experiment data into meaningful results for peers and decision-makers. The UX researcher is tasked with communicating the A/B testing process so that the hypothesis, metrics, and results are meaningful in the context of user experience. Data visualization tools such as Crazy Egg, for example, allow complex click data to be presented compellingly for non-technical colleagues and co-workers. With user data collected and evaluated for this case study, the report went forward to the library web committee that more users clicked through and followed further into the website when the category title was named <span class="rend-i">Services</span>. As a result of this A/B test, the library homepage design was changed to include the <span class="rend-i">Services</span> category title. </p>
</div>
<a name="6"></a><h2>Discussion</h2>
<p>The A/B testing process provides a structure to ask and answer UX questions about library products and services. User behavior insights from this process can be unexpected, and it is crucial that unexpected results remain welcome during the A/B process. In early discussions regarding the experiment detailed in this paper, members of the library web committee favored <span class="rend-i">Learn</span>, the category title that later resulted in the lowest-performing variation during the experiment period. Had A/B testing not been introduced to these design discussions, decision-making regarding the category title would have suffered for its lack of direct user feedback. This example A/B test allowed the library to gather quantitative user data by testing 5 different variations of the website homepage simultaneously. The highest-performing variation improved the library website homepage by creating a user experience that meets user expectations to a greater degree than the previous version of the homepage, as measured by click-through rate, drop-off rate, and homepage-return rate.</p>
<p>The case study presented in this paper describes one website-focused application of A/B testing. The central principles of user-centered experimentation may be applied in creative ways throughout a library’s range of products and services. For example, a library may wish to test the language or design of its email newsletters to improve subscriber engagement and event attendance. Digital signage may also be a subject of A/B testing in a way that reveals the optimal design for gaining the attention of users and communicating relevant information. Mobile app notifications could also be tested with an experiment design that evaluates the effectiveness of various notification messages in generating app activity. </p>
<p>While the A/B testing process offers a powerful and flexible research technique, limitations exist that must be considered. When employed in isolation, A/B testing can lead to incomplete conclusions. Consider a UX research initiative that uses only A/B testing in evaluating the effectiveness of a call-to-action button in a web application. In a narrowly-focused experiment that examines the design of the button and uses click-through rate as the metric, results may skew in favor of a bright or otherwise conspicuous button variation at the expense of other UX problems such as poor information architecture or confusing web copy. UX research into this call-to-action button will benefit from multiple and varied approaches that can together provide a wider scope for the UX problem. As a quantitative evaluation of user behavior, A/B testing itself does not offer qualitative insights that would reveal the purpose or reason of user behaviors. The fundamental value of A/B testing lies in its ability to provide quantitative user insights into known UX problems, problems which themselves must be identified through alternative UX research methodologies.</p>
<p>For these reasons it is necessary to ground experimentation in a wider context by integrating A/B testing into a comprehensive UX research program. Controlled experimentation is most effective in providing user data in complement to additional UX research methods that may include usability studies, paper prototyping, ethnographic studies, data mining, and eye tracking. An A/B test for instance may indicate that users prefer certain library workshop descriptions within a promotional email, as measured by inbound web traffic and subsequent workshop attendance. Usability studies may then further reveal that users prefer those workshops due to the style of description but also because the topics are particularly relevant or interesting. These alternative UX research methods respond to the limitations of A/B testing and together constitute an effective constellation of UX research techniques. </p>
<p>UX research reaches its full potential when quantitative and qualitative methodologies are combined to provide a multi-faceted view of user behaviors, perspectives, and expectations. Such varied UX research programs help realize the concept of perpetual beta, where the design of products and services undergoes a recurring user-centered process of building, testing, analyzing, and iterating. The A/B testing process described in this paper provides one element of a UX research framework that allows for decision-making that is iterative and guided by user behaviors. In sum, A/B testing proved to be an effective method for collecting user data and improving library user experience by offering a method for answering UX questions. The A/B testing process ultimately represents a valuable form of observation, where known UX issues are productively informed by those users who interact directly with the library’s products and services. </p>
<a name="7"></a><h2>References</h2>
<ul class="list-bibl">
<li class="bibl">Allen, J., &amp; Chudley, J. (2012). <span class="rend-i">Smashing UX design: Foundations for designing online user experiences</span> (Vol. 34). John Wiley &amp; Sons.</li>
<li class="bibl">Arento, T. <span class="rend-i">A/B testing in improving conversion on a website: Case: Sanoma Entertainment Oy</span>. (BA Thesis). Available from Theseus.fi database (URN: NBN: fi: Bachelor's 201003235880).</li>
<li class="bibl">Atterer, R., Wnuk, M., &amp; Schmidt, A. (2006, May). Knowing the user's every move: user activity tracking for website usability evaluation and implicit interaction. In <span class="rend-i">Proceedings of the 15th international conference on World Wide Web</span> (pp. 203-212). ACM.</li>
<li class="bibl">Belanche, D., Casaló, L. V., &amp; Guinalíu, M. (2012). Website usability, consumer satisfaction and the intention to use a website: the moderating effect of perceived risk. <span class="rend-i">Journal of retailing and consumer services</span>, 19(1), 124-132. doi:10.1016/j.jretconser.2011.11.001.</li>
<li class="bibl">Bordac, S., &amp; Rainwater, J. (2008). User-centered design in practice: the Brown University experience. <span class="rend-i">Journal of Web Librarianship</span>, 2(2-3), 109-138.</li>
<li class="bibl">Borodovsky, S., &amp; Rosset, S. (2011, December). A/B testing at SweetIM: The importance of proper statistical analysis. In <span class="rend-i">Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference</span> on (pp. 733-740). IEEE.</li>
<li class="bibl">Brooks, P., &amp; Hestnes, B. (2010). User measures of quality of experience: why being objective and quantitative is important. <span class="rend-i">Network, IEEE, 24</span>(2), 8-13.</li>
<li class="bibl">Casaló, L. V., Flavián, C., &amp; Guinalíu, M. (2010). Generating trust and satisfaction in e-services: the impact of usability on consumer behavior. <span class="rend-i">Journal of Relationship Marketing, 9</span>(4), 247-263. doi:10.1080/15332667.2010.522487.</li>
<li class="bibl">Cochran, W. G. (1977). <span class="rend-i">Sampling techniques</span>. New York: Wiley.</li>
<li class="bibl">Cox, D. R., &amp; Reid, N. (2000). <span class="rend-i">The theory of the design of experiments</span>. New York: CRC Press.</li>
<li class="bibl">Connaway, L. S., Hood, E. M., Lanclos, D., White, D., &amp; Le Cornu, A. (2013). User-centered decision making: a new model for developing academic library services and systems. <span class="rend-i">IFLA journal, 39</span>(1), 20-29.</li>
<li class="bibl">Deininger, R. (1960). Human factors engineering studies of the design and use of pushbutton telephone sets. <span class="rend-i">Bell System Technical Journal, 39</span>(4), 995-1012.</li>
<li class="bibl">Fernandez, A., Insfran, E., &amp; Abrahão, S. (2011). Usability evaluation methods for the web: A systematic mapping study. <span class="rend-i">Information and Software Technology,53</span>(8), 789-817. doi:10.1016/j.infsof.2011.02.007</li>
<li class="bibl">Finstad, K. (2010). The usability metric for user experience. <span class="rend-i">Interacting with Computers, 22</span>(5), 323-327. doi:10.1016/j.intcom.2010.04.004</li>
<li class="bibl">Gross, J., &amp; Sheridan, L. (2011). Web scale discovery: The user experience. <span class="rend-i">New library world, 112</span>(5/6), 236-247. doi:10.1108/03074801111136275.</li>
<li class="bibl">Hájek, P, &amp; Stejskal, J. (2012). Analysis of user behavior in a public library using bibliomining. In Oprisan, S., Zaharim, A., Eslamian, S., Jian, MS, Aiub, CA and Azami, A. (Eds.), <span class="rend-i">Advances in Environment, Computational Chemistry and Bioscience</span> (pp. 339-344). WSEAS Press, Montreux, Switzerland. Available at: <span class="ref"><a href="http://www.wseas.us/e-library/conferences/2012/Montreux/BIOCHEMENV/BIOCHEMENV-53.pdf" target="_blank">http://www.wseas.us/e-library/conferences/2012/Montreux/BIOCHEMENV/BIOCHEMENV-53.pdf</a></span>.</li>
<li class="bibl">Herman, J. (2004, April). A process for creating the business case for user experience projects. In <span class="rend-i">CHI'04 extended abstracts on Human factors in computing systems</span> (pp. 1413-1416). ACM.</li>
<li class="bibl">Höchstötter, N., &amp; Lewandowski, D. (2009). What users see–Structures in search engine results pages. <span class="rend-i">Information Sciences, 179</span>(12), 1796-1812.</li>
<li class="bibl">Johnson, R. A., &amp; Wichern, D. W. (2002). <span class="rend-i">Applied multivariate statistical analysis</span> (Vol. 5, No. 8). Upper Saddle River, NJ: Prentice Hall.</li>
<li class="bibl">Kasperek, S., Dorney, E., Williams, B., &amp; O’Brien, M. (2011). A use of space: The unintended messages of academic library web sites. <span class="rend-i">Journal of Web Librarianship, 5</span>(3), 220-248.</li>
<li class="bibl">Khoo, M., Rozaklis, L., &amp; Hall, C. (2012). “A survey of the use of ethnographic methods in the study of libraries and library users." <span class="rend-i">Library &amp; Information Science Research</span> 34(2), 82-91.</li>
<li class="bibl">Kohavi, R., Henne, R. M., &amp; Sommerfield, D. (2007, August). Practical guide to controlled experiments on the web: Listen to your customers not to the hippo. In <span class="rend-i">Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</span> (pp. 959-967). ACM.</li>
<li class="bibl">Kohavi, R., Longbotham, R., Sommerfield, D., &amp; Henne, R. M. (2009). Controlled experiments on the web: survey and practical guide. <span class="rend-i">Data Mining and Knowledge Discovery</span>, 18(1), 140-181. doi:10.1007/s10618-008-0114-1.</li>
<li class="bibl">Kohavi, R., Deng, A., Frasca, B., Longbotham, R., Walker, T., &amp; Xu, Y. (2012, August). Trustworthy online controlled experiments: Five puzzling outcomes explained. In <span class="rend-i">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</span> (pp. 786-794). ACM.</li>
<li class="bibl">Lee, S., &amp; Koubek, R. J. (2010). Understanding user preferences based on usability and aesthetics before and after actual use. <span class="rend-i">Interacting with Computers, 22</span>(6), 530-543. doi:10.1016/j.intcom.2010.05.002.</li>
<li class="bibl">Lee, Y., &amp; Kozar, K. A. (2012). Understanding of website usability: Specifying and measuring constructs and their relationships. <span class="rend-i">Decision Support Systems, 52</span>(2), 450-463. doi:10.1016/j.dss.2011.10.004.</li>
<li class="bibl">Liu, S. (2008). Engaging users: the future of academic library web sites. <span class="rend-i">College &amp; Research Libraries, 69</span>(1), 6-27.</li>
<li class="bibl">Lorigo, L., Haridasan, M., Brynjarsdóttir, H., Xia, L., Joachims, T., Gay, G., &amp; Pan, B. (2008). Eye tracking and online search: Lessons learned and challenges ahead. <span class="rend-i">Journal of the American Society for Information Science and Technology, 59</span>(7), 1041-1052.</li>
<li class="bibl">Lown, C., Sierra, T., &amp; Boyer, J. (2013). How users search the library from a single search box. <span class="rend-i">College &amp; Research Libraries, 74</span>(3), 227-241.</li>
<li class="bibl">Marks, H. M. (2000). <span class="rend-i">The progress of experiment: science and therapeutic reform in the United States, 1900-1990</span>. Cambridge: Cambridge University Press.</li>
<li class="bibl">McGlaughlin, F., Alt, B., &amp; Usborne, N. (2006). The power of small changes tested. <span class="rend-i">Marketing Experiments</span>. Retrieved from <span class="ref"><a href="http://www.marketingexperiments.com/improving-website-conversion/power-small-change.html" target="_blank">http://www.marketingexperiments.com/improving-website-conversion/power-small-change.html</a></span>.</li>
<li class="bibl">Nielsen, J. (1994). Guerrilla HCI: Using discount usability engineering to penetrate the intimidation barrier. <span class="rend-i">Cost-justifying usability</span>, 245-272.</li>
<li class="bibl">Nguyen, T. D., &amp; Nguyen, T. T. (2011). An examination of selected marketing mix elements and brand relationship quality in transition economies: Evidence from Vietnam. <span class="rend-i">Journal of Relationship Marketing, 10</span>(1), 43-56.</li>
<li class="bibl">O'Reilly, T. (2009). <span class="rend-i">What is web 2.0</span>. O'Reilly Media, Inc.</li>
<li class="bibl">Ouellette, D. (2011). Subject guides in academic libraries: A user-centred study of uses and perceptions/Les guides par sujets dans les bibliothèques académiques: Une étude des utilisations et des perceptions centrée sur l'utilisateur. <span class="rend-i">Canadian Journal of Information and Library Science, 35</span>(4), 436-451.</li>
<li class="bibl">Rossi, P. H., &amp; Lipsey, M. W. (2004). <span class="rend-i">Evaluation: A systematic approach</span>. Sage.</li>
<li class="bibl">Sonsteby, A., &amp; DeJonghe, J. (2013). Usability testing, user-centered design, and LibGuides subject guides: A case study. <span class="rend-i">Journal of Web Librarianship, 7</span>(1), 83-94. doi:10.1080/19322909.2013.747366.</li>
<li class="bibl">Swanson, T. A., &amp; Green, J. (2011). Why we are not Google: Lessons from a library web site usability study. <span class="rend-i">The Journal of Academic Librarianship, 37</span>(3), 222-229. doi:10.1016/j.acalib.2011.02.014.</li>
<li class="bibl">Stitz, T., Laster, S., Bove, F. J., &amp; Wise, C. (2011). A path to providing user-centered subject guides. <span class="rend-i">Internet Reference Services Quarterly, 16</span>(4), 183-198. doi: 10.1080/10875301.2011.621819.</li>
<li class="bibl">Tatari, K., Ur-Rehman, S., &amp; Ur-Rehman, W. (2011). Transforming web usability data into web usability information using information architecture concepts &amp; tools. <span class="rend-i">Interdisciplinary Journal of Contemporary Research in Business</span>, 3(4), 703–718.</li>
<li class="bibl">Tang, D., Agarwal, A., O'Brien, D., &amp; Meyer, M. (2010, July). Overlapping experiment infrastructure: More, better, faster experimentation. In <span class="rend-i">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</span> (pp. 17-26). ACM.</li>
<li class="bibl">Visser, E. B., &amp; Weideman, M. (2011). An empirical study on website usability elements and how they affect search engine optimisation. <span class="rend-i">South African Journal of Information Management, 13</span>(1). doi:10.4102/sajim.v13i1.428.</li>
<li class="bibl">Wang, J., &amp; Senecal, S. (2007). Measuring perceived website usability. <span class="rend-i">Journal of Internet Commerce</span>, 6(4), 97–112.</li>
<li class="bibl">Wu, S. K., &amp; Lanclos, D. (2011). Re-imagining the users' experience: An ethnographic approach to web usability and space design. <span class="rend-i">Reference Services Review, 39</span>(3), 369-389.</li>
<li class="bibl">Multivariate testing: A broad sample. (2010, February 4). <span class="rend-i">New Media Age</span>, 18. Retrieved from Academic OneFile.</li>
</ul>
<a name="8"></a><h2>Notes</h2>
<ul class="special-notes"></ul>
<ol>
<li value="1"><p class="numberednote" id="N1"><span class="ref"><a href="http://mcfunley.com/design-for-continuous-experimentation" target="_blank">http://mcfunley.com/design-for-continuous-experimentation</a></span>; <span class="ref"><a href="https://blog.twitter.com/2013/experiments-twitter" target="_blank">https://blog.twitter.com/2013/experiments-twitter</a></span>; <span class="ref"><a href="http://hbr.org/2007/10/the-institutional-yes/ar/1" target="_blank">http://hbr.org/2007/10/the-institutional-yes/ar/1</a></span>; <span class="ref"><a href="http://googleblog.blogspot.com/2009/03/make-sense-of-your-site-tips-for.html" target="_blank">http://googleblog.blogspot.com/2009/03/make-sense-of-your-site-tips-for.html</a></span>.<a href="quod-lib-umich-edu-2350.html#N1-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="2"><p class="numberednote" id="N2"> For Google’s 10-point company philosophy: <span class="ref"><a href="http://www.google.com/about/company/philosophy/" target="_blank">http://www.google.com/about/company/philosophy/</a></span>.<a href="quod-lib-umich-edu-2350.html#N2-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="3"><p class="numberednote" id="N3"> This time period was chosen as a representative snapshot of library web traffic.<a href="quod-lib-umich-edu-2350.html#N3-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="4"><p class="numberednote" id="N4"> Click-through rate is defined as the number of users who visit a page divided by the number of users who click on a specific link, expressed as a percentage. For example, if a page receives 100 visits and a particular link on that page receives 1 click, then the click-through rate for that link would be 1%. <a href="quod-lib-umich-edu-2350.html#N4-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="5"><p class="numberednote" id="N5"> Such “guerilla” testing is an efficient and effective method for conducting rapid qualitative user experience research. See Nielsen (1994) and Allen &amp; Chudley (2012, p.94) for a background and description of “guerilla” testing.<a href="quod-lib-umich-edu-2350.html#N5-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="6"><p class="numberednote" id="N6"> <span class="ref"><a href="http://www.google.com/analytics/" target="_blank">http://www.google.com/analytics/</a></span>; <span class="ref"><a href="http://www.crazyegg.com/" target="_blank">http://www.crazyegg.com/</a></span> <a href="quod-lib-umich-edu-2350.html#N6-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="7"><p class="numberednote" id="N7"> <span class="ref"><a href="http://googleblog.blogspot.com/2009/03/make-sense-of-your-site-tips-for.html" target="_blank">http://googleblog.blogspot.com/2009/03/make-sense-of-your-site-tips-for.html</a></span>; <span class="ref"><a href="https://support.google.com/analytics/answer/2844870" target="_blank">https://support.google.com/analytics/answer/2844870</a></span>.<a href="quod-lib-umich-edu-2350.html#N7-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="8"><p class="numberednote" id="N8">For detailed descriptions of these settings, see <span class="ref"><a href="https://support.google.com/analytics/answer/1745147?hl=en&amp;ref_topic=1745207&amp;rd=1" target="_blank">https://support.google.com/analytics/answer/1745147?hl=en&amp;ref_topic=1745207&amp;rd=1</a></span>; <span class="ref"><a href="https://developers.google.com/analytics/devguides/platform/experiments-overview" target="_blank">https://developers.google.com/analytics/devguides/platform/experiments-overview</a></span>.<a href="quod-lib-umich-edu-2350.html#N8-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
<li value="9"><p class="numberednote" id="N9"> Due to a reporting limitation within Google Analytics, <span class="rend-i">Help</span> category data was incomplete.<a href="quod-lib-umich-edu-2350.html#N9-ptr1"><img class="backToPtr" src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="return to text"></a></p></li>
</ol>
</div>
<a name="bottom"></a><div id="linktotop">
<a href="quod-lib-umich-edu-2350.html#top">Top of page</a>
<a href="quod-lib-umich-edu-2350.html#top"><img src="https://quod.lib.umich.edu/m/mpubs/graphics/arrow_up.gif" alt="Top of page"></a>
</div>
</div></div></div>
<div id="footer">
<div class="logo"><img src="https://quod.lib.umich.edu/m/mpubs/graphics/mpub-logo-bw.png"></div>
<p>Hosted by <a href="http://www.publishing.umich.edu/">Michigan Publishing</a>, a division of the <a href="http://lib.umich.edu">University of Michigan Library</a>.</p>
<p><abbr title="International Standard Serials Number">ISSN</abbr> 2333-3316</p>
<ul>
<li><a href="http://www.weaveux.org/about.html">About</a></li>
<li><a href="http://www.weaveux.org/about.html#who">Editorial Board</a></li>
<li><a href="http://www.weaveux.org/about.html#philosophy">Editorial Philosophy</a></li>
<li><a href="mailto:helloweaveux@umich.edu">Contact</a></li>
<li><a href="https://weaveux.submittable.com/login">Log in</a></li>
</ul>
<ul>
<li><b>Follow us:</b></li>
<li><a href="https://twitter.com/WeaveUX" title="WeaveUX on Twitter">Twitter</a></li>
<li><a href="http://eepurl.com/4AxQ5" title="Subscribe to our email list">Email</a></li>
<li><a href="http://feeds.feedburner.com/weaveux" title="Subscribe to our articles with RSS"><abbr title="Really Simple Syndication">RSS</abbr></a></li>
</ul>
</div>
<script type="text/javascript">
		var _gaq = _gaq || [];
  			_gaq.push(['_setAccount', 'UA-35132797-11']);
  			_gaq.push(['_trackPageview']);

  			(function() {
    			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  			})();
		</script><script type="text/javascript">
	        
	        	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','uga');

        var custom = { 'dimension1' : 'text' };
        custom.dimension2 = 'weave';custom.dimension4 = 'serialarticle';

        var account_codes = 'UA-43730774-1'.split(':');
        $.each(account_codes, function(i, account_code) {
            var name="tracker" + i;
            account_code = $.trim(account_code);
            uga('create', account_code, { name : name });
            uga(name + '.send', 'pageview', custom);
        });
        
        	</script>
</div>
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
</body>
</html>
